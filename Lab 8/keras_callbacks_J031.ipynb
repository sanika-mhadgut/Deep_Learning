{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-callbacks_J031.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanika-mhadgut/Deep_Learning/blob/master/keras_callbacks_J031.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYZKZSsF600j",
        "colab_type": "text"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUXTmWDT6P2U",
        "colab_type": "text"
      },
      "source": [
        "#Name : Sanika Mhadgut\n",
        "#Branch : Btech Data Science sem 6\n",
        "#Roll No: J031"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUJhriKh6tFC",
        "colab_type": "code",
        "outputId": "d8915e42-56b5-4729-847b-8b0dc7a0081d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "\n",
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='elu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='elu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test), callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.2897 - acc: 0.9113 - val_loss: 0.1504 - val_acc: 0.9532\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.95320, saving model to weights-improvement-01-0.95.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.1232 - acc: 0.9619 - val_loss: 0.1005 - val_acc: 0.9682\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.95320 to 0.96820, saving model to weights-improvement-02-0.97.hdf5\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0857 - acc: 0.9732 - val_loss: 0.0968 - val_acc: 0.9686\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.96820 to 0.96860, saving model to weights-improvement-03-0.97.hdf5\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0645 - acc: 0.9798 - val_loss: 0.1131 - val_acc: 0.9657\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.96860\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0511 - acc: 0.9832 - val_loss: 0.0764 - val_acc: 0.9772\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.96860 to 0.97720, saving model to weights-improvement-05-0.98.hdf5\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0423 - acc: 0.9867 - val_loss: 0.0918 - val_acc: 0.9756\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.97720\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0346 - acc: 0.9894 - val_loss: 0.0821 - val_acc: 0.9790\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.97720 to 0.97900, saving model to weights-improvement-07-0.98.hdf5\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0289 - acc: 0.9909 - val_loss: 0.0898 - val_acc: 0.9775\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.97900\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0253 - acc: 0.9919 - val_loss: 0.0828 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.97900 to 0.98030, saving model to weights-improvement-09-0.98.hdf5\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0217 - acc: 0.9928 - val_loss: 0.1157 - val_acc: 0.9741\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f15a5a9fa58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMzADu2469TS",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoint for best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBcdm-kt68s3",
        "colab_type": "code",
        "outputId": "6e8c879a-f8d6-4fe2-c750-a7eb1d46fe7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "source": [
        "import keras\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "\n",
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='elu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='elu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test), callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.2876 - acc: 0.9134 - val_loss: 0.1527 - val_acc: 0.9564\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.95640, saving model to weights.best.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.1227 - acc: 0.9619 - val_loss: 0.1027 - val_acc: 0.9660\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.95640 to 0.96600, saving model to weights.best.hdf5\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0847 - acc: 0.9735 - val_loss: 0.1015 - val_acc: 0.9681\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.96600 to 0.96810, saving model to weights.best.hdf5\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0637 - acc: 0.9795 - val_loss: 0.1025 - val_acc: 0.9696\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.96810 to 0.96960, saving model to weights.best.hdf5\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0514 - acc: 0.9834 - val_loss: 0.0785 - val_acc: 0.9759\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.96960 to 0.97590, saving model to weights.best.hdf5\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0417 - acc: 0.9860 - val_loss: 0.0955 - val_acc: 0.9748\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.97590\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0350 - acc: 0.9887 - val_loss: 0.0917 - val_acc: 0.9784\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.97590 to 0.97840, saving model to weights.best.hdf5\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0289 - acc: 0.9908 - val_loss: 0.1001 - val_acc: 0.9758\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.97840\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0249 - acc: 0.9917 - val_loss: 0.0911 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.97840 to 0.98120, saving model to weights.best.hdf5\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0215 - acc: 0.9931 - val_loss: 0.1057 - val_acc: 0.9772\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f15a0355588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9BfixUL7QyT",
        "colab_type": "text"
      },
      "source": [
        "## Check-pointed Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgU3f6IW7QM8",
        "colab_type": "code",
        "outputId": "731c10fb-eaf1-4e90-9b29-a22c1021fe88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "from keras import models\n",
        "import numpy as np\n",
        "\n",
        "#Create new model\n",
        "model_new = models.Sequential()\n",
        "model_new.add(Dense(512, activation='elu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model_new.add(Dense(256, activation='elu'))\n",
        "model_new.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model_new.load_weights(\"weights.best.hdf5\")\n",
        "\n",
        "# Compile model\n",
        "model_new.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_total=np.concatenate((X_train,X_test))\n",
        "y_total=np.concatenate((y_train,y_test))\n",
        "\n",
        "scores = model_new.evaluate(X_total, y_total, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model_new.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 99.29%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
