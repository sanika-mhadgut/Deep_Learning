{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple-Keras_J031.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanika-mhadgut/Deep_Learning/blob/master/Simple_Keras_J031.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LPdeJnLbhP5",
        "colab_type": "text"
      },
      "source": [
        "#Name : Sanika Mhadgut\n",
        "#Branch : Btech Data Science sem 6\n",
        "#Roll No: J031\n",
        "\n",
        " Keras package in Python\n",
        "\n",
        "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
        "\n",
        "Use Keras if you need a deep learning library that:\n",
        "\n",
        "Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
        "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
        "Runs seamlessly on CPU and GPU.\n",
        "Read the documentation at Keras.io.\n",
        "\n",
        "Keras is compatible with: Python 2.7-3.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5qa9OtRbyqO",
        "colab_type": "text"
      },
      "source": [
        "The core data structure of Keras is a **model**, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6zlphu1bZIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here is the Sequential model:\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHysZQcdcAza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stacking layers is as easy as .add():\n",
        "\n",
        "from keras.layers import Dense\n",
        "\n",
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.add(Dense(units=1, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBcPLRoqd9iH",
        "colab_type": "text"
      },
      "source": [
        "# Compilation\n",
        "\n",
        "Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
        "\n",
        "An optimizer. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class. \n",
        "\n",
        "\n",
        "A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. \n",
        "\n",
        "A list of metrics. For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD6V0BrQcCNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Once your model looks good, configure its learning process with .compile():\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpokwhIudqU7",
        "colab_type": "text"
      },
      "source": [
        "# Specifying the input shape\n",
        "\n",
        "The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n",
        "\n",
        "Pass an input_shape argument to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape, the batch dimension is not included.\n",
        "\n",
        "Some 2D layers, such as Dense, support the specification of their input shape via the argument input_dim, and some 3D temporal layers support the arguments input_dim and input_length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q7KHVXVd47_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate dummy data\n",
        "import numpy as np\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(2, size=(1000, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T2tgwU0e-fX",
        "colab_type": "code",
        "outputId": "fa4adb62-4688-40bf-835c-333682b7ca3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check the shape of inputs\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7wfi1JJfFtu",
        "colab_type": "code",
        "outputId": "bf96db22-178e-454b-f20a-f9ff2eb936df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check the shape of labels\n",
        "labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqYljCVFeLBj",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG96MZWdeP5F",
        "colab_type": "code",
        "outputId": "38cb6434-af25-41e5-cb77-f17352b2313d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, labels, epochs=10, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 1s 543us/step - loss: 8.1944 - acc: 0.4860\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 0s 45us/step - loss: 8.1944 - acc: 0.4860\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 0s 43us/step - loss: 8.1944 - acc: 0.4860\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 0s 50us/step - loss: 8.1944 - acc: 0.4860\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 0s 45us/step - loss: 8.1944 - acc: 0.4860\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 0s 41us/step - loss: 8.1944 - acc: 0.4860\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 0s 41us/step - loss: 8.1944 - acc: 0.4860\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 0s 44us/step - loss: 8.1944 - acc: 0.4860\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 0s 45us/step - loss: 8.1944 - acc: 0.4860\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 0s 44us/step - loss: 8.1944 - acc: 0.4860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f767e7e20b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUSaXF22foxp",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 1: Build a Sequential model with two layers. \n",
        "\n",
        "Layer 1 is dense with 32 nodes and 'relu' activation function. Input dimension is 100.\n",
        "\n",
        "Layer 2 is dense with 10 nodes and 'softmax' activation function.\n",
        "\n",
        "Compile the model with optimizer 'rmsprop',             loss='categorical_crossentropy' and metrics=['accuracy']\n",
        "\n",
        "\n",
        "Generate dummy data using np.random.random 1000, 100 and dummy labels of 10 categories with size 1000,1\n",
        "\n",
        "\n",
        "## Convert labels to categorical one-hot encoding using below statement\n",
        "from keras.utils import to_categorical\n",
        "one_hot_labels = to_categorical(labels, num_classes=10)\n",
        "\n",
        "Finally, Train the model, iterating on the data in batches of 32 samples\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufzot11ZLff3",
        "colab_type": "code",
        "outputId": "df0c14e9-f91d-49cc-db21-c2f01f3b062b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_dim=100))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Generate dummy data\n",
        "import numpy as np\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(10, size=(1000, 1))\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "one_hot_labels = to_categorical(labels, num_classes=10)\n",
        "\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 1s 531us/step - loss: 2.3817 - acc: 0.0960\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 0s 48us/step - loss: 2.3072 - acc: 0.1100\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 0s 43us/step - loss: 2.2940 - acc: 0.1330\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 0s 48us/step - loss: 2.2847 - acc: 0.1390\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 0s 46us/step - loss: 2.2731 - acc: 0.1410\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 0s 47us/step - loss: 2.2648 - acc: 0.1570\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 0s 40us/step - loss: 2.2604 - acc: 0.1610\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 0s 43us/step - loss: 2.2514 - acc: 0.1550\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 0s 42us/step - loss: 2.2427 - acc: 0.1620\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 0s 48us/step - loss: 2.2339 - acc: 0.1720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f767e70c898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJrdnfAMfWbc",
        "colab_type": "text"
      },
      "source": [
        "Double-click <b>here</b> for the solution.\n",
        "\n",
        "<!-- The answer is below:\n",
        "from keras.utils import to_categorical\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_dim=100))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Generate dummy data\n",
        "import numpy as np\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(10, size=(1000, 1))\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "one_hot_labels = to_categorical(labels, num_classes=10)\n",
        "\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, one_hot_labels, epochs=10, batch_size=32)\n",
        "\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVpe2f24jTIW",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 2: Building a Basic Keras Neural Network Sequential Model to classify MNIST dataset\n",
        "\n",
        "First we import package and a set hyperparameter and identify dataset variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl3kYpMSjqUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr-2yrEUjt5Q",
        "colab_type": "text"
      },
      "source": [
        "## Next is a function for outputting some simple (but useful) metadata of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koiwyv_Jj0Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_summary(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Summarize current state of dataset\"\"\"\n",
        "    print('Train images shape:', X_train.shape)\n",
        "    print('Train labels shape:', y_train.shape)\n",
        "    print('Test images shape:', X_test.shape)\n",
        "    print('Test labels shape:', y_test.shape)\n",
        "    print('Train labels:', y_train)\n",
        "    print('Test labels:', y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcHaDkegj-LU",
        "colab_type": "text"
      },
      "source": [
        "## Next we load our dataset (MNIST, using Keras' dataset utilities), and then use the function above to get some dataset metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeMLUCHHkBJy",
        "colab_type": "code",
        "outputId": "63118f56-78f4-4f01-eb0c-9af9d99c74b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Check state of dataset\n",
        "data_summary(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images shape: (60000, 28, 28)\n",
            "Train labels shape: (60000,)\n",
            "Test images shape: (10000, 28, 28)\n",
            "Test labels shape: (10000,)\n",
            "Train labels: [5 0 4 ... 5 6 8]\n",
            "Test labels: [7 2 1 ... 4 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQBMRuA5kKTQ",
        "colab_type": "text"
      },
      "source": [
        "To feed MNIST instances into a neural network, they need to be reshaped, from a 2 dimensional image representation to a single dimension sequence. We also convert our class vector to a binary matrix (using to_categorical). This is accomplished below, after which the same function defined above is called again in order to show the effects of our data reshaping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRwDGnB8kLpm",
        "colab_type": "code",
        "outputId": "58d5dd64-7a53-45fb-a4a6-324a82771494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "# Check state of dataset\n",
        "data_summary(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images shape: (60000, 784)\n",
            "Train labels shape: (60000, 10)\n",
            "Test images shape: (10000, 784)\n",
            "Test labels shape: (10000, 10)\n",
            "Train labels: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n",
            "Test labels: [[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ho-Gqe4elgoK"
      },
      "source": [
        "#Build a Sequential model of Neural Network with three dense layers.\n",
        "\n",
        "\n",
        "\n",
        "1.   Layer 1, 2 and 3 have 512, 256 and 10 nodes respectively. Activation function of first two layers is relu and final layer is softmax.\n",
        "2.   Compile model with optimizer='rmsprop',               loss='categorical_crossentropy' and metrics=['accuracy']\n",
        "3. Train the model with two additional parameters verbose=1 and validation_data=(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkm4jvMtL80x",
        "colab_type": "code",
        "outputId": "cd9099d9-f366-40da-d0c1-d9fe41a0c70d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.2307 - acc: 0.9292 - val_loss: 0.1037 - val_acc: 0.9690\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0854 - acc: 0.9730 - val_loss: 0.0925 - val_acc: 0.9730\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0574 - acc: 0.9823 - val_loss: 0.0828 - val_acc: 0.9754\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0401 - acc: 0.9875 - val_loss: 0.0812 - val_acc: 0.9779\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0289 - acc: 0.9910 - val_loss: 0.1080 - val_acc: 0.9765\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0234 - acc: 0.9926 - val_loss: 0.0945 - val_acc: 0.9783\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0930 - val_acc: 0.9805\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0155 - acc: 0.9953 - val_loss: 0.0925 - val_acc: 0.9809\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.0134 - acc: 0.9957 - val_loss: 0.0928 - val_acc: 0.9820\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.1038 - val_acc: 0.9801\n",
            "Test loss: 0.10384377534471396\n",
            "Test accuracy: 0.9801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R1P-fE9mlkV",
        "colab_type": "text"
      },
      "source": [
        "#Output a summary of the neural network we built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF62Moggmnye",
        "colab_type": "code",
        "outputId": "d9f22aca-f4de-4dc9-cdf1-53807c766466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Summary of neural network\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_22 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 535,818\n",
            "Trainable params: 535,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3r9ODkFmqwX",
        "colab_type": "text"
      },
      "source": [
        "# Visualize the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF5PEZWPmtvf",
        "colab_type": "code",
        "outputId": "443bc8df-edd2-40e1-c65a-43ae679a192e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "# Output network visualization\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"352pt\" viewBox=\"0.00 0.00 188.00 264.00\" width=\"251pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 260)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 184,-260 184,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140147057019760 -->\n<g class=\"node\" id=\"node1\">\n<title>140147057019760</title>\n<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 180,-255.5 180,-219.5 0,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-233.8\">dense_22_input: InputLayer</text>\n</g>\n<!-- 140147057018920 -->\n<g class=\"node\" id=\"node2\">\n<title>140147057018920</title>\n<polygon fill=\"none\" points=\"33,-146.5 33,-182.5 147,-182.5 147,-146.5 33,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-160.8\">dense_22: Dense</text>\n</g>\n<!-- 140147057019760&#45;&gt;140147057018920 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140147057019760-&gt;140147057018920</title>\n<path d=\"M90,-219.4551C90,-211.3828 90,-201.6764 90,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"93.5001,-192.5903 90,-182.5904 86.5001,-192.5904 93.5001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140147057018808 -->\n<g class=\"node\" id=\"node3\">\n<title>140147057018808</title>\n<polygon fill=\"none\" points=\"33,-73.5 33,-109.5 147,-109.5 147,-73.5 33,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-87.8\">dense_23: Dense</text>\n</g>\n<!-- 140147057018920&#45;&gt;140147057018808 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140147057018920-&gt;140147057018808</title>\n<path d=\"M90,-146.4551C90,-138.3828 90,-128.6764 90,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"93.5001,-119.5903 90,-109.5904 86.5001,-119.5904 93.5001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140146906400472 -->\n<g class=\"node\" id=\"node4\">\n<title>140146906400472</title>\n<polygon fill=\"none\" points=\"33,-.5 33,-36.5 147,-36.5 147,-.5 33,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-14.8\">dense_24: Dense</text>\n</g>\n<!-- 140147057018808&#45;&gt;140146906400472 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140147057018808-&gt;140146906400472</title>\n<path d=\"M90,-73.4551C90,-65.3828 90,-55.6764 90,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"93.5001,-46.5903 90,-36.5904 86.5001,-46.5904 93.5001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XeWXc0ft6fO",
        "colab_type": "text"
      },
      "source": [
        "#Exercise: Train a Sequential Neural network on CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPWjNVe9uCnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import packages\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjXMCDsxuHAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#change shape from image to vector\n",
        "X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "#preprocess\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "#change labels from numeric to one hot encoded\n",
        "Y_train = to_categorical(y_train, 10)\n",
        "Y_test =  to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SV6PPCHu6iy",
        "colab_type": "text"
      },
      "source": [
        "# Build sequential model \n",
        "\n",
        "\n",
        "\n",
        "1.   Sequential model with four layers having 1024,512,512,10 nodes respectively\n",
        "2.   Input shape = 3072\n",
        "3.   Activation of first three layers is relu and final is softmax\n",
        "4.   Compile model using loss='categorical_crossentropy', optimizer='adam' and metrics=['accuracy']\n",
        "5. Train model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t51-PrDOvK0R",
        "colab_type": "text"
      },
      "source": [
        "Double-click <b>here</b> for the solution.\n",
        "\n",
        "<!--\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(3072, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        " \n",
        "\n",
        "# training\n",
        "history = model.fit(X_train, Y_train,\n",
        "                        batch_size=128,\n",
        "                        nb_epoch=10,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxAE5ryN6cAz",
        "colab_type": "code",
        "outputId": "d161e835-4be2-4853-d693-4bd398e7ebe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(3072, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        " \n",
        "\n",
        "# training\n",
        "history = model.fit(X_train, Y_train,\n",
        "                        batch_size=128,\n",
        "                        nb_epoch=10,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 39s 785us/step - loss: 1.8845 - acc: 0.3180 - val_loss: 1.7117 - val_acc: 0.3794\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 38s 762us/step - loss: 1.6693 - acc: 0.3990 - val_loss: 1.6197 - val_acc: 0.4167\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 38s 761us/step - loss: 1.5802 - acc: 0.4306 - val_loss: 1.5636 - val_acc: 0.4418\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 38s 756us/step - loss: 1.5089 - acc: 0.4584 - val_loss: 1.5195 - val_acc: 0.4571\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 38s 760us/step - loss: 1.4661 - acc: 0.4741 - val_loss: 1.4486 - val_acc: 0.4905\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 38s 759us/step - loss: 1.4315 - acc: 0.4896 - val_loss: 1.4597 - val_acc: 0.4746\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 38s 763us/step - loss: 1.3874 - acc: 0.5051 - val_loss: 1.4721 - val_acc: 0.4748\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 38s 760us/step - loss: 1.3541 - acc: 0.5160 - val_loss: 1.4403 - val_acc: 0.4912\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 38s 763us/step - loss: 1.3189 - acc: 0.5276 - val_loss: 1.3951 - val_acc: 0.5100\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 38s 765us/step - loss: 1.2817 - acc: 0.5413 - val_loss: 1.4167 - val_acc: 0.4996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81PL1bVxZRZ",
        "colab_type": "text"
      },
      "source": [
        "# Plot accuracy and loss after model is trained. \n",
        "\n",
        "(Caution: Will not work unless previous step is performed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptYUQKCWwTUf",
        "colab_type": "code",
        "outputId": "cc673705-be7e-4c3f-d595-6c3135442f7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1JOX52A01Dc",
        "colab_type": "text"
      },
      "source": [
        "# Improving accuracy on Iris dataset\n",
        "\n",
        "\n",
        "\n",
        "1.   Find the accuracy of the below model\n",
        "2.   Improve accuracy by increasing layers, epochs, optimizer or loss metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN6DlpbYytgU",
        "colab_type": "code",
        "outputId": "ff834e39-c4f3-44b9-8c46-bc200f09f29c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "iris_data = load_iris()\n",
        "\t\n",
        "x = iris_data.data\n",
        "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
        "\n",
        "# One Hot encode the class labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y_)\n",
        "\n",
        "\n",
        "# Split the data for training and testing\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.01)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(10, input_shape=(4,), activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=200)\n",
        "\n",
        "# Test on unseen data\n",
        "\n",
        "results = model.evaluate(test_x, test_y)\n",
        "\n",
        "print('Final test set loss: {:4f}'.format(results[0]))\n",
        "print('Final test set accuracy: {:4f}'.format(results[1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            " - 1s - loss: 1.2797 - acc: 0.3311\n",
            "Epoch 2/200\n",
            " - 0s - loss: 1.0822 - acc: 0.3311\n",
            "Epoch 3/200\n",
            " - 0s - loss: 0.9934 - acc: 0.3311\n",
            "Epoch 4/200\n",
            " - 0s - loss: 0.9261 - acc: 0.4324\n",
            "Epoch 5/200\n",
            " - 0s - loss: 0.8663 - acc: 0.6689\n",
            "Epoch 6/200\n",
            " - 0s - loss: 0.8106 - acc: 0.6689\n",
            "Epoch 7/200\n",
            " - 0s - loss: 0.7536 - acc: 0.6689\n",
            "Epoch 8/200\n",
            " - 0s - loss: 0.6984 - acc: 0.6689\n",
            "Epoch 9/200\n",
            " - 0s - loss: 0.6451 - acc: 0.6689\n",
            "Epoch 10/200\n",
            " - 0s - loss: 0.6019 - acc: 0.6757\n",
            "Epoch 11/200\n",
            " - 0s - loss: 0.5634 - acc: 0.6689\n",
            "Epoch 12/200\n",
            " - 0s - loss: 0.5315 - acc: 0.7027\n",
            "Epoch 13/200\n",
            " - 0s - loss: 0.5035 - acc: 0.7095\n",
            "Epoch 14/200\n",
            " - 0s - loss: 0.4829 - acc: 0.7500\n",
            "Epoch 15/200\n",
            " - 0s - loss: 0.4614 - acc: 0.8041\n",
            "Epoch 16/200\n",
            " - 0s - loss: 0.4453 - acc: 0.8243\n",
            "Epoch 17/200\n",
            " - 0s - loss: 0.4313 - acc: 0.8986\n",
            "Epoch 18/200\n",
            " - 0s - loss: 0.4202 - acc: 0.8649\n",
            "Epoch 19/200\n",
            " - 0s - loss: 0.4083 - acc: 0.9189\n",
            "Epoch 20/200\n",
            " - 0s - loss: 0.3979 - acc: 0.8919\n",
            "Epoch 21/200\n",
            " - 0s - loss: 0.3881 - acc: 0.9527\n",
            "Epoch 22/200\n",
            " - 0s - loss: 0.3790 - acc: 0.9189\n",
            "Epoch 23/200\n",
            " - 0s - loss: 0.3738 - acc: 0.9054\n",
            "Epoch 24/200\n",
            " - 0s - loss: 0.3639 - acc: 0.9324\n",
            "Epoch 25/200\n",
            " - 0s - loss: 0.3563 - acc: 0.9527\n",
            "Epoch 26/200\n",
            " - 0s - loss: 0.3436 - acc: 0.9189\n",
            "Epoch 27/200\n",
            " - 0s - loss: 0.3343 - acc: 0.9595\n",
            "Epoch 28/200\n",
            " - 0s - loss: 0.3274 - acc: 0.9662\n",
            "Epoch 29/200\n",
            " - 0s - loss: 0.3210 - acc: 0.9257\n",
            "Epoch 30/200\n",
            " - 0s - loss: 0.3155 - acc: 0.9595\n",
            "Epoch 31/200\n",
            " - 0s - loss: 0.3039 - acc: 0.9459\n",
            "Epoch 32/200\n",
            " - 0s - loss: 0.2968 - acc: 0.9527\n",
            "Epoch 33/200\n",
            " - 0s - loss: 0.2899 - acc: 0.9595\n",
            "Epoch 34/200\n",
            " - 0s - loss: 0.2799 - acc: 0.9662\n",
            "Epoch 35/200\n",
            " - 0s - loss: 0.2769 - acc: 0.9730\n",
            "Epoch 36/200\n",
            " - 0s - loss: 0.2704 - acc: 0.9730\n",
            "Epoch 37/200\n",
            " - 0s - loss: 0.2626 - acc: 0.9527\n",
            "Epoch 38/200\n",
            " - 0s - loss: 0.2532 - acc: 0.9865\n",
            "Epoch 39/200\n",
            " - 0s - loss: 0.2470 - acc: 0.9662\n",
            "Epoch 40/200\n",
            " - 0s - loss: 0.2400 - acc: 0.9662\n",
            "Epoch 41/200\n",
            " - 0s - loss: 0.2345 - acc: 0.9865\n",
            "Epoch 42/200\n",
            " - 0s - loss: 0.2293 - acc: 0.9797\n",
            "Epoch 43/200\n",
            " - 0s - loss: 0.2259 - acc: 0.9662\n",
            "Epoch 44/200\n",
            " - 0s - loss: 0.2170 - acc: 0.9730\n",
            "Epoch 45/200\n",
            " - 0s - loss: 0.2119 - acc: 0.9595\n",
            "Epoch 46/200\n",
            " - 0s - loss: 0.2113 - acc: 0.9595\n",
            "Epoch 47/200\n",
            " - 0s - loss: 0.2044 - acc: 0.9730\n",
            "Epoch 48/200\n",
            " - 0s - loss: 0.2011 - acc: 0.9730\n",
            "Epoch 49/200\n",
            " - 0s - loss: 0.1935 - acc: 0.9797\n",
            "Epoch 50/200\n",
            " - 0s - loss: 0.1906 - acc: 0.9730\n",
            "Epoch 51/200\n",
            " - 0s - loss: 0.1845 - acc: 0.9730\n",
            "Epoch 52/200\n",
            " - 0s - loss: 0.1807 - acc: 0.9595\n",
            "Epoch 53/200\n",
            " - 0s - loss: 0.1778 - acc: 0.9797\n",
            "Epoch 54/200\n",
            " - 0s - loss: 0.1736 - acc: 0.9797\n",
            "Epoch 55/200\n",
            " - 0s - loss: 0.1708 - acc: 0.9797\n",
            "Epoch 56/200\n",
            " - 0s - loss: 0.1639 - acc: 0.9865\n",
            "Epoch 57/200\n",
            " - 0s - loss: 0.1613 - acc: 0.9730\n",
            "Epoch 58/200\n",
            " - 0s - loss: 0.1595 - acc: 0.9797\n",
            "Epoch 59/200\n",
            " - 0s - loss: 0.1644 - acc: 0.9662\n",
            "Epoch 60/200\n",
            " - 0s - loss: 0.1527 - acc: 0.9797\n",
            "Epoch 61/200\n",
            " - 0s - loss: 0.1480 - acc: 0.9797\n",
            "Epoch 62/200\n",
            " - 0s - loss: 0.1458 - acc: 0.9797\n",
            "Epoch 63/200\n",
            " - 0s - loss: 0.1439 - acc: 0.9730\n",
            "Epoch 64/200\n",
            " - 0s - loss: 0.1424 - acc: 0.9730\n",
            "Epoch 65/200\n",
            " - 0s - loss: 0.1414 - acc: 0.9797\n",
            "Epoch 66/200\n",
            " - 0s - loss: 0.1350 - acc: 0.9797\n",
            "Epoch 67/200\n",
            " - 0s - loss: 0.1352 - acc: 0.9797\n",
            "Epoch 68/200\n",
            " - 0s - loss: 0.1300 - acc: 0.9797\n",
            "Epoch 69/200\n",
            " - 0s - loss: 0.1304 - acc: 0.9797\n",
            "Epoch 70/200\n",
            " - 0s - loss: 0.1256 - acc: 0.9797\n",
            "Epoch 71/200\n",
            " - 0s - loss: 0.1290 - acc: 0.9730\n",
            "Epoch 72/200\n",
            " - 0s - loss: 0.1299 - acc: 0.9662\n",
            "Epoch 73/200\n",
            " - 0s - loss: 0.1241 - acc: 0.9797\n",
            "Epoch 74/200\n",
            " - 0s - loss: 0.1180 - acc: 0.9797\n",
            "Epoch 75/200\n",
            " - 0s - loss: 0.1216 - acc: 0.9662\n",
            "Epoch 76/200\n",
            " - 0s - loss: 0.1164 - acc: 0.9797\n",
            "Epoch 77/200\n",
            " - 0s - loss: 0.1166 - acc: 0.9797\n",
            "Epoch 78/200\n",
            " - 0s - loss: 0.1132 - acc: 0.9797\n",
            "Epoch 79/200\n",
            " - 0s - loss: 0.1126 - acc: 0.9797\n",
            "Epoch 80/200\n",
            " - 0s - loss: 0.1118 - acc: 0.9797\n",
            "Epoch 81/200\n",
            " - 0s - loss: 0.1188 - acc: 0.9797\n",
            "Epoch 82/200\n",
            " - 0s - loss: 0.1109 - acc: 0.9662\n",
            "Epoch 83/200\n",
            " - 0s - loss: 0.1068 - acc: 0.9797\n",
            "Epoch 84/200\n",
            " - 0s - loss: 0.1086 - acc: 0.9797\n",
            "Epoch 85/200\n",
            " - 0s - loss: 0.1096 - acc: 0.9662\n",
            "Epoch 86/200\n",
            " - 0s - loss: 0.1131 - acc: 0.9662\n",
            "Epoch 87/200\n",
            " - 0s - loss: 0.1027 - acc: 0.9865\n",
            "Epoch 88/200\n",
            " - 0s - loss: 0.1037 - acc: 0.9865\n",
            "Epoch 89/200\n",
            " - 0s - loss: 0.0997 - acc: 0.9865\n",
            "Epoch 90/200\n",
            " - 0s - loss: 0.0994 - acc: 0.9865\n",
            "Epoch 91/200\n",
            " - 0s - loss: 0.0981 - acc: 0.9797\n",
            "Epoch 92/200\n",
            " - 0s - loss: 0.0971 - acc: 0.9797\n",
            "Epoch 93/200\n",
            " - 0s - loss: 0.0963 - acc: 0.9865\n",
            "Epoch 94/200\n",
            " - 0s - loss: 0.0969 - acc: 0.9797\n",
            "Epoch 95/200\n",
            " - 0s - loss: 0.0935 - acc: 0.9797\n",
            "Epoch 96/200\n",
            " - 0s - loss: 0.0930 - acc: 0.9797\n",
            "Epoch 97/200\n",
            " - 0s - loss: 0.0929 - acc: 0.9797\n",
            "Epoch 98/200\n",
            " - 0s - loss: 0.0934 - acc: 0.9797\n",
            "Epoch 99/200\n",
            " - 0s - loss: 0.0924 - acc: 0.9797\n",
            "Epoch 100/200\n",
            " - 0s - loss: 0.0903 - acc: 0.9865\n",
            "Epoch 101/200\n",
            " - 0s - loss: 0.0913 - acc: 0.9797\n",
            "Epoch 102/200\n",
            " - 0s - loss: 0.0934 - acc: 0.9865\n",
            "Epoch 103/200\n",
            " - 0s - loss: 0.0896 - acc: 0.9797\n",
            "Epoch 104/200\n",
            " - 0s - loss: 0.0895 - acc: 0.9865\n",
            "Epoch 105/200\n",
            " - 0s - loss: 0.0922 - acc: 0.9797\n",
            "Epoch 106/200\n",
            " - 0s - loss: 0.0923 - acc: 0.9730\n",
            "Epoch 107/200\n",
            " - 0s - loss: 0.0903 - acc: 0.9797\n",
            "Epoch 108/200\n",
            " - 0s - loss: 0.0859 - acc: 0.9730\n",
            "Epoch 109/200\n",
            " - 0s - loss: 0.0939 - acc: 0.9662\n",
            "Epoch 110/200\n",
            " - 0s - loss: 0.0956 - acc: 0.9595\n",
            "Epoch 111/200\n",
            " - 0s - loss: 0.0867 - acc: 0.9730\n",
            "Epoch 112/200\n",
            " - 0s - loss: 0.0869 - acc: 0.9865\n",
            "Epoch 113/200\n",
            " - 0s - loss: 0.0851 - acc: 0.9730\n",
            "Epoch 114/200\n",
            " - 0s - loss: 0.0823 - acc: 0.9730\n",
            "Epoch 115/200\n",
            " - 0s - loss: 0.0842 - acc: 0.9797\n",
            "Epoch 116/200\n",
            " - 0s - loss: 0.0839 - acc: 0.9797\n",
            "Epoch 117/200\n",
            " - 0s - loss: 0.0879 - acc: 0.9865\n",
            "Epoch 118/200\n",
            " - 0s - loss: 0.0797 - acc: 0.9865\n",
            "Epoch 119/200\n",
            " - 0s - loss: 0.0824 - acc: 0.9730\n",
            "Epoch 120/200\n",
            " - 0s - loss: 0.0816 - acc: 0.9797\n",
            "Epoch 121/200\n",
            " - 0s - loss: 0.0845 - acc: 0.9730\n",
            "Epoch 122/200\n",
            " - 0s - loss: 0.0837 - acc: 0.9730\n",
            "Epoch 123/200\n",
            " - 0s - loss: 0.0831 - acc: 0.9730\n",
            "Epoch 124/200\n",
            " - 0s - loss: 0.0839 - acc: 0.9730\n",
            "Epoch 125/200\n",
            " - 0s - loss: 0.0802 - acc: 0.9730\n",
            "Epoch 126/200\n",
            " - 0s - loss: 0.0808 - acc: 0.9797\n",
            "Epoch 127/200\n",
            " - 0s - loss: 0.0794 - acc: 0.9797\n",
            "Epoch 128/200\n",
            " - 0s - loss: 0.0767 - acc: 0.9865\n",
            "Epoch 129/200\n",
            " - 0s - loss: 0.0756 - acc: 0.9797\n",
            "Epoch 130/200\n",
            " - 0s - loss: 0.0743 - acc: 0.9865\n",
            "Epoch 131/200\n",
            " - 0s - loss: 0.0777 - acc: 0.9797\n",
            "Epoch 132/200\n",
            " - 0s - loss: 0.0795 - acc: 0.9797\n",
            "Epoch 133/200\n",
            " - 0s - loss: 0.0806 - acc: 0.9730\n",
            "Epoch 134/200\n",
            " - 0s - loss: 0.0771 - acc: 0.9797\n",
            "Epoch 135/200\n",
            " - 0s - loss: 0.0733 - acc: 0.9865\n",
            "Epoch 136/200\n",
            " - 0s - loss: 0.0729 - acc: 0.9797\n",
            "Epoch 137/200\n",
            " - 0s - loss: 0.0741 - acc: 0.9797\n",
            "Epoch 138/200\n",
            " - 0s - loss: 0.0752 - acc: 0.9730\n",
            "Epoch 139/200\n",
            " - 0s - loss: 0.0755 - acc: 0.9797\n",
            "Epoch 140/200\n",
            " - 0s - loss: 0.0747 - acc: 0.9730\n",
            "Epoch 141/200\n",
            " - 0s - loss: 0.0719 - acc: 0.9797\n",
            "Epoch 142/200\n",
            " - 0s - loss: 0.0736 - acc: 0.9797\n",
            "Epoch 143/200\n",
            " - 0s - loss: 0.0759 - acc: 0.9797\n",
            "Epoch 144/200\n",
            " - 0s - loss: 0.0706 - acc: 0.9865\n",
            "Epoch 145/200\n",
            " - 0s - loss: 0.0747 - acc: 0.9865\n",
            "Epoch 146/200\n",
            " - 0s - loss: 0.0724 - acc: 0.9797\n",
            "Epoch 147/200\n",
            " - 0s - loss: 0.0706 - acc: 0.9797\n",
            "Epoch 148/200\n",
            " - 0s - loss: 0.0717 - acc: 0.9865\n",
            "Epoch 149/200\n",
            " - 0s - loss: 0.0697 - acc: 0.9797\n",
            "Epoch 150/200\n",
            " - 0s - loss: 0.0704 - acc: 0.9797\n",
            "Epoch 151/200\n",
            " - 0s - loss: 0.0719 - acc: 0.9797\n",
            "Epoch 152/200\n",
            " - 0s - loss: 0.0819 - acc: 0.9797\n",
            "Epoch 153/200\n",
            " - 0s - loss: 0.0688 - acc: 0.9797\n",
            "Epoch 154/200\n",
            " - 0s - loss: 0.0699 - acc: 0.9797\n",
            "Epoch 155/200\n",
            " - 0s - loss: 0.0766 - acc: 0.9797\n",
            "Epoch 156/200\n",
            " - 0s - loss: 0.0712 - acc: 0.9797\n",
            "Epoch 157/200\n",
            " - 0s - loss: 0.0705 - acc: 0.9797\n",
            "Epoch 158/200\n",
            " - 0s - loss: 0.0744 - acc: 0.9662\n",
            "Epoch 159/200\n",
            " - 0s - loss: 0.0706 - acc: 0.9797\n",
            "Epoch 160/200\n",
            " - 0s - loss: 0.0681 - acc: 0.9797\n",
            "Epoch 161/200\n",
            " - 0s - loss: 0.0723 - acc: 0.9797\n",
            "Epoch 162/200\n",
            " - 0s - loss: 0.0709 - acc: 0.9797\n",
            "Epoch 163/200\n",
            " - 0s - loss: 0.0770 - acc: 0.9797\n",
            "Epoch 164/200\n",
            " - 0s - loss: 0.0699 - acc: 0.9797\n",
            "Epoch 165/200\n",
            " - 0s - loss: 0.0688 - acc: 0.9797\n",
            "Epoch 166/200\n",
            " - 0s - loss: 0.0731 - acc: 0.9662\n",
            "Epoch 167/200\n",
            " - 0s - loss: 0.0714 - acc: 0.9797\n",
            "Epoch 168/200\n",
            " - 0s - loss: 0.0751 - acc: 0.9865\n",
            "Epoch 169/200\n",
            " - 0s - loss: 0.0744 - acc: 0.9730\n",
            "Epoch 170/200\n",
            " - 0s - loss: 0.0695 - acc: 0.9797\n",
            "Epoch 171/200\n",
            " - 0s - loss: 0.0661 - acc: 0.9730\n",
            "Epoch 172/200\n",
            " - 0s - loss: 0.0705 - acc: 0.9797\n",
            "Epoch 173/200\n",
            " - 0s - loss: 0.0717 - acc: 0.9797\n",
            "Epoch 174/200\n",
            " - 0s - loss: 0.0695 - acc: 0.9797\n",
            "Epoch 175/200\n",
            " - 0s - loss: 0.0684 - acc: 0.9797\n",
            "Epoch 176/200\n",
            " - 0s - loss: 0.0767 - acc: 0.9932\n",
            "Epoch 177/200\n",
            " - 0s - loss: 0.0647 - acc: 0.9797\n",
            "Epoch 178/200\n",
            " - 0s - loss: 0.0657 - acc: 0.9730\n",
            "Epoch 179/200\n",
            " - 0s - loss: 0.0709 - acc: 0.9730\n",
            "Epoch 180/200\n",
            " - 0s - loss: 0.0729 - acc: 0.9730\n",
            "Epoch 181/200\n",
            " - 0s - loss: 0.0680 - acc: 0.9730\n",
            "Epoch 182/200\n",
            " - 0s - loss: 0.0680 - acc: 0.9797\n",
            "Epoch 183/200\n",
            " - 0s - loss: 0.0651 - acc: 0.9932\n",
            "Epoch 184/200\n",
            " - 0s - loss: 0.0656 - acc: 0.9797\n",
            "Epoch 185/200\n",
            " - 0s - loss: 0.0679 - acc: 0.9730\n",
            "Epoch 186/200\n",
            " - 0s - loss: 0.0693 - acc: 0.9797\n",
            "Epoch 187/200\n",
            " - 0s - loss: 0.0724 - acc: 0.9797\n",
            "Epoch 188/200\n",
            " - 0s - loss: 0.0734 - acc: 0.9527\n",
            "Epoch 189/200\n",
            " - 0s - loss: 0.0686 - acc: 0.9797\n",
            "Epoch 190/200\n",
            " - 0s - loss: 0.0672 - acc: 0.9730\n",
            "Epoch 191/200\n",
            " - 0s - loss: 0.0674 - acc: 0.9797\n",
            "Epoch 192/200\n",
            " - 0s - loss: 0.0658 - acc: 0.9797\n",
            "Epoch 193/200\n",
            " - 0s - loss: 0.0664 - acc: 0.9865\n",
            "Epoch 194/200\n",
            " - 0s - loss: 0.0648 - acc: 0.9730\n",
            "Epoch 195/200\n",
            " - 0s - loss: 0.0640 - acc: 0.9797\n",
            "Epoch 196/200\n",
            " - 0s - loss: 0.0691 - acc: 0.9865\n",
            "Epoch 197/200\n",
            " - 0s - loss: 0.0693 - acc: 0.9797\n",
            "Epoch 198/200\n",
            " - 0s - loss: 0.0676 - acc: 0.9797\n",
            "Epoch 199/200\n",
            " - 0s - loss: 0.0645 - acc: 0.9865\n",
            "Epoch 200/200\n",
            " - 0s - loss: 0.0633 - acc: 0.9730\n",
            "2/2 [==============================] - 0s 129ms/step\n",
            "Final test set loss: 0.001131\n",
            "Final test set accuracy: 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rkZZKV4yV9U",
        "colab_type": "text"
      },
      "source": [
        "#Thank you for completing this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl-88q4Mu-OH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<!--\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_shape=(3072, )))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # training\n",
        "    history = model.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "-->"
      ]
    }
  ]
}
